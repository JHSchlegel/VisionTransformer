data:
  dataset: 'cifar10' # 'cifar10' or 'cifar100'
  train_batch_size: 512 # batch size for training
  val_batch_size: 1024 # batch size for validation
  test_batch_size: 1024 # batch size for testing
  val_perc_size: 0.2 # percentage of training data to use for validation

model:
  image_size: 32 # image size
  patch_size: 4 # patch size
  in_channels: 3 # number of input channels
  embed_dim: 512 # embedding dimension
  depth: 4 # number of transformer blocks
  num_heads: 8 # number of heads in multi-head attention
  mlp_ratio: 5 # hidden layer size = mlp_ratio * embed_dim
  dropout: 0.2 # dropout rate

training:
  num_epochs: 200 # number of training epochs
  use_early_stopping: True # whether to use early stopping
  early_stopping_patience: 20 # patience for early stopping
  early_stopping_tol: 1e-7 # tolerance for early stopping
  save_model_every: 10 # checkpoint frequency
  use_wandb: True # whether to use Weights & Biases
  wandb_project: 'vit' # Weights & Biases project name
  num_samples: 5 # number of samples to visualize
  top_k: 5 # number of top-k predictions to visualize

optimizer:
  lr: 0.0003 # learning rate
  weight_decay: 0.0001 # weight decay

scheduler:
  T_0: 10 # number of iterations for the first cycle
  T_mult: 2 # factor by which the cycle length (initially T_0) is increased after each cycle

hydra:
  run:
    dir: '../results/vit/${data.dataset}'